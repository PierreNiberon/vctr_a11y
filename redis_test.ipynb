{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval Augmented Generation\n",
    "\n",
    "Vector store: Redis\n",
    "Model: T5-Large\n",
    "\n",
    "*Add docker compose to set up Redis.\n",
    "*The model used for storing the vectors and vectorizing the question can be different from the model doing the text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needed if downloading large model to switch to the HDD, bigscience/T0 for example\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'D:\\python\\.cache'\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from redis import from_url, Redis\n",
    "\n",
    "# Connection to the redis instance\n",
    "REDIS_URL = 'redis://localhost:6379'\n",
    "client = from_url(REDIS_URL)\n",
    "client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "r = Redis()\n",
    "total_keys = r.dbsize()\n",
    "print(total_keys)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the embeddings from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m T5Tokenizer, T5Model\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mt5-large\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "import torch\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "model = T5Model.from_pretrained('t5-large')\n",
    "\n",
    "def get_vector(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Pass the input to the encoder\n",
    "        encoder_outputs = model.get_encoder()(input_ids)\n",
    "        \n",
    "    # Retrieve the last hidden state of the encoder\n",
    "    hidden_state = encoder_outputs.last_hidden_state\n",
    "\n",
    "    # Compute the mean over the sequence dimension\n",
    "    return hidden_state.mean(dim=1).numpy().flatten().tolist()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of documents\n",
    "text_1 = \"\"\"Japan narrowly escapes recession ...\"\"\"\n",
    "text_2 = \"\"\"Dibaba breaks 5,000m world record ...\"\"\"\n",
    "text_3 = \"\"\"Google's toolbar sparks concern ...\"\"\"\n",
    "text_4 = \"\"\"Web accessibility, or eAccessibility, is the inclusive practice of ensuring there are no barriers that prevent interaction with, or access to, websites on the World Wide Web by people with physical disabilities, situational disabilities, and socio-economic restrictions on bandwidth and speed.\"\"\"\n",
    "\n",
    "# Builds the json with the content in natural language and the vectors\n",
    "doc_1 = {\"content\": text_1, \"vector\": get_vector(text_1)}\n",
    "doc_2 = {\"content\": text_2, \"vector\": get_vector(text_2)}\n",
    "doc_3 = {\"content\": text_3, \"vector\": get_vector(text_3)}\n",
    "doc_4 = {\"content\": text_4, \"vector\": get_vector(text_4)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the index in Redis (schema). The creation is dynamic on the DIM so that it can adapt to other model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped index\n",
      "created Index\n"
     ]
    }
   ],
   "source": [
    "from redis.commands.search.field import TextField, VectorField\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "\n",
    "schema = [ VectorField('$.vector', \n",
    "            \"FLAT\", \n",
    "            {   \"TYPE\": 'FLOAT32', \n",
    "                \"DIM\": len(doc_1['vector']), \n",
    "                \"DISTANCE_METRIC\": \"COSINE\"\n",
    "            },  as_name='vector' ),\n",
    "            TextField('$.content', as_name='content')\n",
    "        ]\n",
    "idx_def = IndexDefinition(index_type=IndexType.JSON, prefix=['doc:'])\n",
    "try: \n",
    "    client.ft('idx').dropindex()\n",
    "    print(\"dropped index\")\n",
    "except:\n",
    "    pass\n",
    "client.ft('idx').create_index(schema, definition=idx_def)\n",
    "print(\"created Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads the document into Redis.\n",
    "# Careful with the prefix in the name as Redis use that to associate a document and an index.\n",
    "client.json().set('doc:1', '$', doc_1)\n",
    "client.json().set('doc:2', '$', doc_2)\n",
    "client.json().set('doc:3', '$', doc_3)\n",
    "client.json().set('doc:4', '$', doc_4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval of context and text generation.\n",
    "\n",
    "The process is the following:\n",
    "Get an input question from a user.\n",
    "Vectorize this question with the same model we vectorize the documents.\n",
    "Do a semantic search (KNN vector distance) or Hybrid search (Vector distances + full text search) between the documents and the question. This is a feature of Redis so no need to implement the algorithm.\n",
    "Get back the natural language part of the document.\n",
    "Insert that as a context into the prompt for the model to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip to Table of Contents - Skip to main content Introduction to RGAA RGAA companion guide Technical reference Criteria - current page Glossary Particular cases Technical notes Baseline References RGAA 3 2016 - Criteria - English translation The RGAA is the French government's General Accessibility Reference for Administrations. It is meant to provide a way to check conformity to WCAG 2.0. Table of Contents How to use the RGAA Images Frames Colors Multimedia Tables Links Scripts Mandatory elements Information structure Presentation of information Forms Navigation Consultation How to use the RGAA The RGAA applies to any HTML content (HTML4, XHTML1, HTML5). For some tests, a reference baseline is used. This baseline takes into account a set of assistive technologies, browsers and operating systems, on which the accessibility of JavaScript-based interface components must be tested, among others. A detailed description is provided here: Baseline. Important notice regarding HTML content prior to HTML5 specification When the HTML code of the page is not HTML5, the HTML5 elements (tags and attributes) required by a criterion or test, are not applicable. Every other criteria or tests remain applicable, including those related to ARIA attributes, states or properties. The following criteria and tests are not applicable: Criterion 1.10; Criterion 9.2; Test 11.10.1 (condition 2, relative to the HTML5 attribute required). Validation process For each criterion, compliance is defined as follows: Conforming (C): all applicable tests are passed Non Conforming (NC): at least one applicable test is failed Not Applicable (NA): there is no content targeted by the criterion. The\n",
      "question: What criteria about images? context: Skip to Table of Contents - Skip to main content Introduction to RGAA RGAA companion guide Technical reference Criteria - current page Glossary Particular cases Technical notes Baseline References RGAA 3 2016 - Criteria - English translation The RGAA is the French government's General Accessibility Reference for Administrations. It is meant to provide a way to check conformity to WCAG 2.0. Table of Contents How to use the RGAA Images Frames Colors Multimedia Tables Links Scripts Mandatory elements Information structure Presentation of information Forms Navigation Consultation How to use the RGAA The RGAA applies to any HTML content (HTML4, XHTML1, HTML5). For some tests, a reference baseline is used. This baseline takes into account a set of assistive technologies, browsers and operating systems, on which the accessibility of JavaScript-based interface components must be tested, among others. A detailed description is provided here: Baseline. Important notice regarding HTML content prior to HTML5 specification When the HTML code of the page is not HTML5, the HTML5 elements (tags and attributes) required by a criterion or test, are not applicable. Every other criteria or tests remain applicable, including those related to ARIA attributes, states or properties. The following criteria and tests are not applicable: Criterion 1.10; Criterion 9.2; Test 11.10.1 (condition 2, relative to the HTML5 attribute required). Validation process For each criterion, compliance is defined as follows: Conforming (C): all applicable tests are passed Non Conforming (NC): at least one applicable test is failed Not Applicable (NA): there is no content targeted by the criterion. The\n",
      "[{'generated_text': 'Frames Colors Multimedia Tables Links Scripts Mandatory elements'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from redis.commands.search.query import Query\n",
    "import numpy as np\n",
    "# Reimport Query from redis and see if numpy is really needed here?\n",
    "\n",
    "# First, load the model and tokenizer manually\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\")\n",
    "\n",
    "\n",
    "def get_answer(client, idx, question, model, tokenizer):\n",
    "    # Get vector for the question, need to be converted tobytes for redis\n",
    "    vec = np.array(get_vector(question), dtype=np.float32).tobytes()\n",
    "        \n",
    "    \n",
    "    # Define the search query\n",
    "    q = Query('*=>[KNN 1 @vector $query_vec AS vector_score]').return_fields('content').dialect(2)    \n",
    "    \n",
    "    # Define query parameters\n",
    "    params = {\"query_vec\": vec}\n",
    "\n",
    "    # Execute the search query\n",
    "    results = client.ft(idx).search(q, query_params=params)\n",
    "\n",
    "    if len(results.docs) == 0:\n",
    "        return \"No relevant documents found in database. Please seek professional help.\"\n",
    "    else:\n",
    "        # Retrieve the content of the most relevant document\n",
    "        document = results.docs[0]['content'].strip()\n",
    "        print(document)\n",
    "        # Build the prompt\n",
    "        \n",
    "        prompt = f\"question: {question} context: {document}\"\n",
    "        print(prompt)\n",
    "        # Then, pass the model and tokenizer to the pipeline function\n",
    "        text2text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "        result = text2text_generator(prompt)\n",
    "        print(result)\n",
    "\n",
    "# You can call the function like this\n",
    "question = \"What criteria about images?\"\n",
    "get_answer(client, 'idx', question, model, tokenizer)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinecone_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
