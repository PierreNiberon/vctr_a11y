{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script to split document based on token length to not overload the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def chunk_text(filename, tokenizer, max_length):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"The file {filename} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Add a space before each period to ensure sentence boundary is a single token\n",
    "    text = text.replace(\".\", \" .\")\n",
    "\n",
    "    tokens = tokenizer.encode(text, truncation=False, return_tensors='pt')[0]\n",
    "    chunks = []\n",
    "\n",
    "    # Initialize the start index\n",
    "    start = 0\n",
    "\n",
    "    # Loop over the tokens and create chunks\n",
    "    while start < len(tokens):\n",
    "        if len(tokens[start:]) > max_length:\n",
    "            end = start + max_length\n",
    "            # Make sure the end index is not in the middle of a sentence\n",
    "            while tokens[end] != tokenizer.encode(\".\", add_special_tokens=False)[0]:\n",
    "                end -= 1\n",
    "            end += 1  # Include the period in the current chunk\n",
    "        else:\n",
    "            end = len(tokens)\n",
    "\n",
    "        # Create a chunk and add it to the list of chunks\n",
    "        chunk = tokens[start:end]\n",
    "        chunks.append(tokenizer.decode(chunk))\n",
    "\n",
    "        # Update the start index\n",
    "        start = end\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\"\n",
    "Le handicap est défini comme : toute limitation d’activité ou restriction de participation à la vie en société subie dans son environnement par une personne en raison d’une altération substantielle, durable ou définitive d’une ou plusieurs fonctions physiques, sensorielles, mentales, cognitives ou psychiques, d’un polyhandicap ou d’un trouble de santé invalidant (article L. 114 du code de l’action sociale et des familles).\n",
    "\n",
    "L’accessibilité numérique consiste à rendre les services de communication au public en ligne accessibles aux personnes handicapées, c’est-à-dire :\n",
    "\n",
    "perceptibles : par exemple, faciliter la perception visuelle et auditive du contenu par l’utilisateur ; proposer des équivalents textuels à tout contenu non textuel ; créer un contenu qui puisse être présenté de différentes manières sans perte d’information ni de structure (par exemple avec une mise en page simplifiée) ;\n",
    "utilisables : par exemple, fournir à l’utilisateur des éléments d’orientation pour naviguer, trouver le contenu ; rendre toutes les fonctionnalités accessibles au clavier ; laisser à l’utilisateur suffisamment de temps pour lire et utiliser le contenu ; ne pas concevoir de contenu susceptible de provoquer des crises d’épilepsie ;\n",
    "compréhensibles : par exemple, faire en sorte que les pages fonctionnent de manière prévisible ; aider l’utilisateur à corriger les erreurs de saisie.\n",
    "robustes : par exemple, optimiser la compatibilité avec les utilisations actuelles et futures, y compris avec les technologies d’assistance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35680 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "filename = \"vctr.txt\"\n",
    "chunks = chunk_text(filename, tokenizer, 412)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "import torch\n",
    "from redis import from_url\n",
    "from redis.commands.search.field import TextField, VectorField\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "\n",
    "\n",
    "# Connection to the redis instance\n",
    "REDIS_URL = 'redis://localhost:6379'\n",
    "client = from_url(REDIS_URL)\n",
    "client.ping()\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "model = T5Model.from_pretrained('t5-large')\n",
    "\n",
    "def get_vector(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Pass the input to the encoder\n",
    "        encoder_outputs = model.get_encoder()(input_ids)\n",
    "        \n",
    "    # Retrieve the last hidden state of the encoder\n",
    "    hidden_state = encoder_outputs.last_hidden_state\n",
    "\n",
    "    # Compute the mean over the sequence dimension\n",
    "    return hidden_state.mean(dim=1).numpy().flatten().tolist()\n",
    "\n",
    "\n",
    "# Builds the json with the content in natural language and the vectors\n",
    "for count, chunk in enumerate(chunks):\n",
    "    doc_json = {\"content\": chunk, \"vector\": get_vector(chunk)}\n",
    "    \n",
    "    # Creates the schema in redis based on first doc\n",
    "    if count == 0:\n",
    "        schema = [ VectorField('$.vector', \n",
    "                    \"FLAT\", \n",
    "                    {   \"TYPE\": 'FLOAT32', \n",
    "                        \"DIM\": len(doc_json['vector']), \n",
    "                        \"DISTANCE_METRIC\": \"COSINE\"\n",
    "                    },  as_name='vector' ),\n",
    "                    TextField('$.content', as_name='content')\n",
    "                ]\n",
    "        idx_def = IndexDefinition(index_type=IndexType.JSON, prefix=['doc:'])\n",
    "        # try: \n",
    "        #     client.ft('idx').dropindex()\n",
    "        #     print(\"dropped index\")\n",
    "        # except:\n",
    "        #     pass\n",
    "        # client.ft('idx').create_index(schema, definition=idx_def)\n",
    "        # print(\"created Index\")\n",
    "\n",
    "    # Loads the document into Redis.\n",
    "    # Careful with the prefix in the name as Redis use that to associate a document and an index.\n",
    "    doc_name = \"doc:\" + str(count)\n",
    "    client.json().set(doc_name, '$', doc_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinecone_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
